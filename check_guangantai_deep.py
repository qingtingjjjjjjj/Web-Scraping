#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹ live.txt ä¸­â€œæ¸¯æ¾³å°,#genre#â€å’Œâ€œå°æ¹¾å°,#genre#â€åˆ†ç»„çš„ç›´æ’­æºåšæ·±åº¦æµ‹é€Ÿï¼ˆæ›´å‡†ç¡®ï¼‰
é€»è¾‘ï¼š
  - å°è¯•è¿æ¥å¹¶è¯»å–å°‘é‡æ•°æ®ï¼ˆåˆ¤æ–­å¯æ’­æ”¾ï¼‰
  - é¦–åŒ…å“åº”æ—¶é—´æµ‹é€Ÿ
  - è‹¥å¤±è´¥è‡ªåŠ¨é‡è¯•3æ¬¡
  - æŒ‡å®šåŸŸåå…æµ‹è¯•
  - æŒ‰ç›´æ’­æºé“¾æ¥å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„æ¡ç›®ï¼Œæ ¼å¼ä¿æŒ "èŠ‚ç›®åç§°,ç›´æ’­æºé“¾æ¥"ï¼‰
  - ä»…å½“æµ‹é€Ÿç»“æœæœ‰å˜åŒ–æ—¶ï¼Œæ›´æ–° live.txt ä¸­å¯¹åº”åˆ†ç»„å†…å®¹
"""

import os
import time
import requests
from concurrent.futures import ThreadPoolExecutor, as_completed

LIVE_FILE = "live.txt"
TARGET_GROUPS = ["æ¸¯æ¾³å°,#genre#", "å°æ¹¾å°,#genre#"]  # å¤šåˆ†ç»„æ”¯æŒ
TIMEOUT = 10
MAX_WORKERS = 8
RETRY_COUNT = 3

# å…æµ‹è¯•åŸŸååˆ—è¡¨
IMMUNE_DOMAINS = ["bxtv.3a.ink"]


def parse_live_file(filepath):
    """è¯»å–æ‰€æœ‰åˆ†ç»„å¹¶è¿”å›å®Œæ•´æ–‡æœ¬åŠå„åˆ†ç»„å†…å®¹"""
    with open(filepath, encoding="utf-8", errors="ignore") as f:
        lines = [line.rstrip("\n") for line in f]

    groups = []
    current_group = None
    current_entries = []

    for line in lines:
        if line.endswith("#genre#"):
            if current_group:
                groups.append((current_group, current_entries))
            current_group = line
            current_entries = []
        elif current_group:
            if "," in line:
                current_entries.append(line)
    if current_group:
        groups.append((current_group, current_entries))

    return lines, groups


def deep_test_once(name, url):
    """å•æ¬¡æµ‹é€Ÿ"""
    start = time.time()
    try:
        with requests.get(url, stream=True, timeout=TIMEOUT, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
        }) as r:
            first_chunk_time = None
            for chunk in r.iter_content(chunk_size=2048):
                if chunk:
                    first_chunk_time = round(time.time() - start, 3)
                    break
            if r.status_code == 200 and first_chunk_time:
                return {"name": name, "url": url, "status": "OK", "time": first_chunk_time}
            else:
                return {"name": name, "url": url, "status": f"HTTP {r.status_code}", "time": None}
    except Exception as e:
        return {"name": name, "url": url, "status": f"Fail: {e.__class__.__name__}", "time": None}


def deep_test(name, url):
    """æ·±åº¦æµ‹é€Ÿï¼Œå¤±è´¥é‡è¯•æœ€å¤š3æ¬¡ï¼ŒæŒ‡å®šåŸŸåå…æµ‹è¯•"""
    # æ£€æŸ¥æ˜¯å¦å±äºå…æµ‹è¯•åŸŸå
    if any(domain in url for domain in IMMUNE_DOMAINS):
        print(f"ğŸ’¡ {name} å±äºå…æµ‹è¯•åŸŸåï¼Œç›´æ¥æ ‡è®°ä¸º OK")
        return {"name": name, "url": url, "status": "OK (å…æµ‹è¯•)", "time": 0.0}

    # æ™®é€šæ·±åº¦æµ‹é€Ÿ
    for attempt in range(1, RETRY_COUNT + 1):
        result = deep_test_once(name, url)
        if result["status"] == "OK":
            if attempt > 1:
                result["status"] += f" (retry {attempt-1})"
            return result
        else:
            print(f"  â³ [{attempt}/{RETRY_COUNT}] {name} æµ‹é€Ÿå¤±è´¥ï¼Œé‡è¯•ä¸­...")
            time.sleep(1)
    # å…¨éƒ¨å¤±è´¥
    result["status"] += " (all retries failed)"
    return result


def update_live_file(lines, new_entries_dict):
    """æ›¿æ¢å¤šä¸ªç›®æ ‡åˆ†ç»„å†…å®¹"""
    updated_lines = []
    inside_target = None

    for line in lines:
        # è¿›å…¥åˆ†ç»„ï¼ˆè‹¥ä¸ºç›®æ ‡åˆ†ç»„åˆ™å†™å…¥æ–°å†…å®¹ï¼‰
        if line in TARGET_GROUPS:
            updated_lines.append(line)
            inside_target = line
            updated_lines.extend(new_entries_dict.get(line, []))
            continue

        # åœ¨æ›¿æ¢åˆ†ç»„å†…å®¹æ—¶ï¼Œè·³è¿‡åŸæœ‰åˆ†ç»„é¡¹ç›´åˆ°é‡åˆ°ä¸‹ä¸€ä¸ªåˆ†ç»„æ ‡é¢˜ï¼ˆæˆ–æ–‡ä»¶æœ«å°¾ï¼‰
        if inside_target:
            if line.endswith("#genre#") and line not in TARGET_GROUPS:
                updated_lines.append(line)
                inside_target = None
            else:
                # è·³è¿‡æ—§åˆ†ç»„ä¸­åŸæœ‰çš„ name,url è¡Œ
                continue
        else:
            updated_lines.append(line)

    return updated_lines


def process_group(group_name, items):
    """å¤„ç†å•ä¸ªåˆ†ç»„"""
    entries = []
    for line in items:
        if "," in line:
            name, url = line.split(",", 1)
            if url.startswith("http"):
                # ä¿ç•™åŸå§‹ name å’ŒåŸå§‹ urlï¼ˆå»æ‰é¦–å°¾ç©ºç™½ï¼‰
                entries.append((name.strip(), url.strip(), line))  # ä¿ç•™åŸè¡Œä»¥ä¾¿æ ¼å¼å®Œå…¨ä¸å˜

    if not entries:
        print(f"âš ï¸ åˆ†ç»„ {group_name} æœªæ‰¾åˆ°å¯æµ‹é€Ÿå†…å®¹ã€‚")
        return group_name, []

    # === ä¸¥æ ¼æŒ‰ URL å­—ç¬¦ä¸²å»é‡ï¼ˆåªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„æ¡ç›®ï¼‰ï¼Œå¹¶ä¿ç•™åŸå§‹æ ¼å¼ "èŠ‚ç›®åç§°,ç›´æ’­æºé“¾æ¥" ===
    seen_urls = set()
    unique_entries = []
    for name, url, original_line in entries:
        normalized_url = url  # ä¸¥æ ¼æŒ‰åŸå§‹å­—ç¬¦ä¸²æ¯”å¯¹ï¼ˆå·² stripï¼‰ï¼Œä¸åšå…¶å®ƒè§„èŒƒåŒ–
        if normalized_url not in seen_urls:
            seen_urls.add(normalized_url)
            # ä¿ç•™ name å’Œ urlï¼ˆä½œä¸º tupleï¼‰ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹è¡Œæ ¼å¼ç”¨äºæœ€ç»ˆå†™å›
            unique_entries.append((name, url, original_line))
        else:
            # æ—¥å¿—æç¤ºè¢«å»æ‰ä½†ä¸æ”¹å˜ä»»ä½•å…¶å®ƒä¿¡æ¯
            print(f"ğŸ—‘ï¸ å»é‡ï¼ˆä¿ç•™å…ˆå‡ºç°çš„ï¼‰ï¼š{name} ({url})")

    print(f"\nğŸš€ å‘ç° {len(unique_entries)} æ¡ {group_name.replace(',#genre#','')} ç›´æ’­æºï¼ˆå»é‡åï¼‰ï¼Œå¼€å§‹æ·±åº¦æµ‹é€Ÿ...\n")

    results = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # æ³¨æ„ï¼šæäº¤ç»™æµ‹é€Ÿçš„ä»æ˜¯ name ä¸ urlï¼ˆä¸æ”¹å˜åŸå§‹æ ¼å¼ï¼‰
        futures = [executor.submit(deep_test, name, url) for name, url, _ in unique_entries]
        for fut in as_completed(futures):
            res = fut.result()
            results.append(res)
            status = "âœ…" if res["status"].startswith("OK") else "âŒ"
            print(f"{status} {res['name']} - {res['url']} [{res['status']}] {res['time']}s")

    # æœ€ç»ˆå†™å›çš„æ ¼å¼ä»ç„¶ä½¿ç”¨ "èŠ‚ç›®åç§°,ç›´æ’­æºé“¾æ¥"
    ok_list = [f"{r['name']},{r['url']}" for r in results if r["status"].startswith("OK")]
    print(f"âœ… {group_name.replace(',#genre#','')} å¯ç”¨æºï¼š{len(ok_list)} æ¡ã€‚\n")
    return group_name, ok_list


def main():
    if not os.path.exists(LIVE_FILE):
        print(f"âŒ æœªæ‰¾åˆ° {LIVE_FILE}")
        return

    lines, groups = parse_live_file(LIVE_FILE)
    group_dict = dict(groups)

    new_entries_dict = {}
    updated = False

    for group_name in TARGET_GROUPS:
        if group_name in group_dict:
            name, ok_list = process_group(group_name, group_dict[group_name])
            if ok_list:
                old_list = group_dict[group_name]
                if set(ok_list) != set(old_list):
                    new_entries_dict[group_name] = ok_list
                    updated = True
                else:
                    print(f"âš™ï¸ {group_name.replace(',#genre#','')} å†…å®¹æ— å˜åŒ–ï¼Œä¸æ›´æ–°ã€‚")
        else:
            print(f"âš ï¸ æœªæ‰¾åˆ°åˆ†ç»„ï¼š{group_name}")

    if not updated:
        print("ğŸŸ¢ æ‰€æœ‰åˆ†ç»„å†…å®¹å‡æ— å˜åŒ–ï¼Œä¸æ›´æ–° live.txtã€‚")
        return

    new_lines = update_live_file(lines, new_entries_dict)
    with open(LIVE_FILE, "w", encoding="utf-8") as f:
        f.write("\n".join(new_lines) + "\n")

    print(f"\nâœ… å·²æ›´æ–° {LIVE_FILE}ï¼Œå…±æ›´æ–° {len(new_entries_dict)} ä¸ªåˆ†ç»„ã€‚")


if __name__ == "__main__":
    main()
