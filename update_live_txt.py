import requests
import os
import re
import shutil
import time
from datetime import datetime, timedelta, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed

# ===== é¢œè‰²å®šä¹‰ï¼ˆæ§åˆ¶å°æ—¥å¿—ï¼‰=====
RED = "\033[91m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
BLUE = "\033[94m"
RESET = "\033[0m"

live_file = "live.txt"
backup_dir = "backup"

# ===== æ¥å£åœ°å€ =====
sources = {
    "TXT": "https://raw.githubusercontent.com/lucheng7996/TE/refs/heads/main/outfiles/beijing_cucc.txt",
    "M3U": "https://raw.githubusercontent.com/develop202/migu_video/refs/heads/main/interface.txt"
}

# ===== å·¥å…·å‡½æ•° =====
def simplify_name(name: str) -> str:
    name = re.sub(r'HD', '', name, flags=re.IGNORECASE)
    name = re.sub(r'BRTV', '', name, flags=re.IGNORECASE)
    name = name.strip()
    cctv_match = re.match(r"CCTV[-]?(\d+)", name, re.IGNORECASE)
    if cctv_match:
        return f"CCTV{cctv_match.group(1)}"
    return name

def fetch_source(name, url, color, retries=3):
    for attempt in range(1, retries+1):
        try:
            resp = requests.get(url, timeout=15)
            resp.raise_for_status()
            lines = resp.text.splitlines()
            print(f"{color}[{name}] æŠ“å–æˆåŠŸï¼Œå…± {len(lines)} è¡Œ{RESET}")
            return lines
        except Exception as e:
            print(f"{RED}[{name}] ç¬¬ {attempt} æ¬¡æŠ“å–å¤±è´¥: {e}{RESET}")
            time.sleep(1)
    return []

# ===== åˆå§‹åŒ–åˆ†ç»„ =====
yangshi, weishi = [], []
yangshi_detail, weishi_detail = [], []

# ===== å¹¶å‘æŠ“å– M3U å’Œ TXT =====
start_time = time.time()
with ThreadPoolExecutor(max_workers=2) as executor:
    future_to_source = {
        executor.submit(fetch_source, name, url, BLUE if name=="TXT" else YELLOW): name
        for name, url in sources.items()
    }
    results = {}
    for future in as_completed(future_to_source):
        name = future_to_source[future]
        results[name] = future.result()

lines_m3u = results.get("M3U", [])
lines_txt = results.get("TXT", [])

# ===== è§£æ M3U =====
current_group, current_name = None, None
for line in lines_m3u:
    if line.startswith("#EXTINF"):
        current_name = simplify_name(line.split(",")[-1].strip())
        if "å¤®è§†" in line or current_name.startswith("CCTV"):
            current_group = "yangshi"
        elif "å«è§†" in line:
            current_group = "weishi"
        else:
            current_group = None
    elif line.startswith("http") and current_group and current_name:
        record = f"{current_name},{line.strip()}"
        if current_group == "yangshi":
            yangshi.append(record)
            yangshi_detail.append(f"{current_name} -> {line.strip()} (M3U)")
        elif current_group == "weishi":
            weishi.append(record)
            weishi_detail.append(f"{current_name} -> {line.strip()} (M3U)")

# ===== è§£æ TXT =====
for line in lines_txt:
    if "," in line:
        name, url = line.split(",", 1)
        name = simplify_name(name)
        record = f"{name},{url.strip()}"
        if "CCTV" in name:
            yangshi.append(record)
            yangshi_detail.append(f"{name} -> {url.strip()} (TXT)")
        elif "å«è§†" in name:
            weishi.append(record)
            weishi_detail.append(f"{name} -> {url.strip()} (TXT)")

if not yangshi and not weishi:
    print(f"{RED}æŠ“å–åˆ°çš„ç›´æ’­æºä¸ºç©ºï¼Œä¿ç•™æ—§çš„ live.txt æ–‡ä»¶{RESET}")
    exit(0)

# ===== è¯»å–åŸæœ‰ live.txt =====
if os.path.exists(live_file):
    with open(live_file, "r", encoding="utf-8") as f:
        old_lines = f.read().splitlines()
else:
    old_lines = []

# ===== åˆ†ç»„æ ‡ç­¾ =====
yangshi_tag = "å¤®è§†é¢‘é“,#genre#"
weishi_tag = "å«è§†é¢‘é“,#genre#"

def update_group(existing_lines, tag, new_records):
    if not new_records:
        return existing_lines, [], []  # æ–°å¢ã€æ›´æ–°åˆ—è¡¨

    if tag not in existing_lines:
        return existing_lines + ["", tag] + new_records + [""], new_records, []

    idx = existing_lines.index(tag) + 1
    end_idx = idx
    while end_idx < len(existing_lines) and existing_lines[end_idx].strip() != "" and not existing_lines[end_idx].endswith(",#genre#"):
        end_idx += 1

    old_group_lines = existing_lines[idx:end_idx]
    old_names = {line.split(",")[0]: line for line in old_group_lines}
    new_names = {rec.split(",")[0]: rec for rec in new_records}

    added = [rec for name, rec in new_names.items() if name not in old_names]
    updated = [rec for name, rec in new_names.items() if name in old_names]

    filtered_old_lines = [line for name, line in old_names.items() if name not in new_names]
    updated_group = added + updated + filtered_old_lines
    return existing_lines[:idx] + updated_group + existing_lines[end_idx:], added, updated

# ===== æ›´æ–°åˆ†ç»„ =====
lines_after_yangshi, y_added, y_updated = update_group(old_lines, yangshi_tag, yangshi)
lines_after_weishi, w_added, w_updated = update_group(lines_after_yangshi, weishi_tag, weishi)

# ===== å¤‡ä»½ live.txt =====
if not os.path.exists(backup_dir):
    os.makedirs(backup_dir)
if os.path.exists(live_file):
    shutil.copy(live_file, os.path.join(backup_dir, f"live_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"))

# ===== å†™å› live.txt =====
new_content = "\n".join(lines_after_weishi)
if not os.path.exists(live_file) or open(live_file, encoding="utf-8").read() != new_content:
    with open(live_file, "w", encoding="utf-8") as f:
        f.write(new_content)
    live_updated = True
else:
    live_updated = False

# ===== ç»Ÿè®¡æŠ“å–æ•°é‡ =====
txt_count = len(lines_txt)
m3u_count = len(lines_m3u)
total_count = len(lines_after_weishi)

# ===== æ›´æ–° README.mdï¼ˆå½©è‰² Markdown è¡¨æ ¼ï¼‰=====
beijing_tz = timezone(timedelta(hours=8))
timestamp = datetime.now(beijing_tz).strftime("%Y-%m-%d %H:%M:%S")
header = f"## âœ¨äº {timestamp} æ›´æ–°"
subline = f"**ğŸ‰æœ€æ–°å¯ç”¨IPTVæºï¼ŒTXT: {txt_count} æ¡ï¼ŒM3U: {m3u_count} æ¡ï¼Œæ€»è®¡: {total_count} æ¡**"

def md_table(title, items, color=""):
    if not items:
        return ""
    rows = "\n".join([f"| {rec.split(',')[0]} | {rec.split(',')[1]} |" for rec in items])
    table = f"### {title}\n\n| é¢‘é“å | é“¾æ¥ |\n| --- | --- |\n{rows}\n"
    return table

readme_update_lines = [
    header,
    subline,
    md_table("å¤®è§†é¢‘é“æ–°å¢", y_added, GREEN),
    md_table("å¤®è§†é¢‘é“æ›´æ–°", y_updated, GREEN),
    md_table("å«è§†é¢‘é“æ–°å¢", w_added, YELLOW),
    md_table("å«è§†é¢‘é“æ›´æ–°", w_updated, YELLOW),
    ""
]

if os.path.exists("README.md"):
    with open("README.md", "r", encoding="utf-8") as f:
        readme_lines = f.read().splitlines()

    new_readme = []
    skip_block = False
    for line in readme_lines:
        if line.startswith("## âœ¨äº "):
            skip_block = True
            continue
        if skip_block:
            if line.strip() == "" or line.startswith("## "):
                skip_block = False
            else:
                continue
        new_readme.append(line)

    backup_file = os.path.join(backup_dir, f"README_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md")
    shutil.copy("README.md", backup_file)

    with open("README.md", "w", encoding="utf-8") as f:
        f.write("\n".join(readme_update_lines + new_readme))

# ===== æ§åˆ¶å°æ—¥å¿— =====
def log_channels(name, added, updated, detail_list, color):
    print(f"{color}{name}: æ–°å¢ {len(added)} æ¡ | æ›´æ–° {len(updated)} æ¡{RESET}")
    for i, rec in enumerate(detail_list, 1):
        print(f"{color}{i}. {rec}{RESET}")

log_channels("å¤®è§†é¢‘é“", y_added, y_updated, yangshi_detail, GREEN)
log_channels("å«è§†é¢‘é“", w_added, w_updated, weishi_detail, YELLOW)

end_time = time.time()
print(f"{RED}æ›´æ–°å®Œæˆ âœ… è€—æ—¶ {end_time - start_time:.2f} ç§’{RESET}")
if live_updated:
    print(f"{RED}live.txt å·²æ›´æ–°ï¼Œå¤‡ä»½å·²ä¿å­˜åˆ° {backup_dir}{RESET}")
else:
    print(f"{YELLOW}live.txt æ— å˜åŒ–ï¼Œæœªæ›´æ–°{RESET}")
