import aiohttp
import asyncio
import datetime
import os
import requests
from collections import defaultdict

LIVE_KEYWORD = "PopC"
OUTPUT_FILE = "PopC_live_top3.txt"
GITHUB_TOKEN = ""  # å¯é€‰ï¼Œå¢åŠ  API é™é¢
PER_PAGE = 30       # æ¯é¡µæœç´¢æ•°é‡
MAX_PAGES = 3       # æœ€å¤šæŠ“å–é¡µæ•°

HEADERS = {"Authorization": f"token {GITHUB_TOKEN}"} if GITHUB_TOKEN else {}

# 1ï¸âƒ£ GitHub æœç´¢å¤šé¡µ
def fetch_github_search():
    files = []
    for page in range(1, MAX_PAGES + 1):
        url = f"https://api.github.com/search/code?q={LIVE_KEYWORD}+in:file&per_page={PER_PAGE}&page={page}"
        try:
            r = requests.get(url, headers=HEADERS, timeout=10)
            r.raise_for_status()
            data = r.json()
            items = data.get("items", [])
            for item in items:
                raw_url = item["html_url"].replace("github.com", "raw.githubusercontent.com").replace("/blob/", "/")
                files.append(raw_url)
        except Exception as e:
            print(f"âŒ GitHub æœç´¢å¤±è´¥: {e}")
            continue
    return files

# 2ï¸âƒ£ å¼‚æ­¥æŠ“å–æ–‡ä»¶å†…å®¹
async def fetch_file(session, url):
    try:
        async with session.get(url, timeout=10) as resp:
            resp.raise_for_status()
            text = await resp.text()
            return [line for line in text.splitlines() if LIVE_KEYWORD in line]
    except:
        return []

async def fetch_all_files(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_file(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        lines = [line for sublist in results for line in sublist]
        return lines

# 3ï¸âƒ£ å¼‚æ­¥æµ‹é€Ÿå•ä¸ª URL
async def test_url(session, name, url):
    try:
        start = asyncio.get_event_loop().time()
        async with session.get(url, timeout=5) as resp:
            await resp.read()
            latency = asyncio.get_event_loop().time() - start
            return (name, url, latency)
    except:
        return (name, url, float("inf"))

# 4ï¸âƒ£ å¼‚æ­¥æµ‹é€Ÿæ‰€æœ‰ PopC
async def test_popc_live(lines):
    results = []
    async with aiohttp.ClientSession() as session:
        tasks = []
        for line in lines:
            if "," in line:
                name, url = line.split(",", 1)
                tasks.append(test_url(session, name, url))
        results = await asyncio.gather(*tasks)
    # æŒ‰èŠ‚ç›®åˆ†ç»„ï¼Œå– Top3 æœ€å¿«
    grouped = defaultdict(list)
    for name, url, latency in results:
        grouped[name].append((url, latency))
    top3_results = []
    for name, urls in grouped.items():
        urls.sort(key=lambda x: x[1])
        for url, latency in urls[:3]:
            top3_results.append((name, url, latency))
    # å…¨éƒ¨æŒ‰å»¶è¿Ÿæ’åº
    top3_results.sort(key=lambda x: x[2])
    return top3_results

# 5ï¸âƒ£ ä¿å­˜æ–‡ä»¶
def save_file(results):
    if not os.path.exists(os.path.dirname(OUTPUT_FILE)) and os.path.dirname(OUTPUT_FILE):
        os.makedirs(os.path.dirname(OUTPUT_FILE))
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for name, url, latency in results:
            latency_str = "timeout" if latency == float("inf") else f"{latency:.2f}s"
            f.write(f"{name},{url},å»¶è¿Ÿ:{latency_str}\n")
        f.write(f"# æ›´æ–°æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    print(f"âœ… æ–‡ä»¶å·²ç”Ÿæˆ: {OUTPUT_FILE}")

# 6ï¸âƒ£ ä¸»å‡½æ•°
async def main():
    urls = fetch_github_search()
    print(f"ğŸ” å…±æ‰¾åˆ° {len(urls)} ä¸ª GitHub æ–‡ä»¶")
    lines = await fetch_all_files(urls)
    print(f"ğŸ“„ å…±æŠ“å– {len(lines)} æ¡ PopC ç›´æ’­æº")
    results = await test_popc_live(lines)
    save_file(results)

if __name__ == "__main__":
    asyncio.run(main())
